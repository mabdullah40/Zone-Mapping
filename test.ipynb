{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"name\": \"Amigu-Production\",\n",
      "    \"pause_reason\": null,\n",
      "    \"syntax\": \"sql\",\n",
      "    \"paused\": 0,\n",
      "    \"view_only\": false,\n",
      "    \"type\": \"rds_mysql\",\n",
      "    \"id\": 18\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Billing Production\",\n",
      "    \"pause_reason\": null,\n",
      "    \"syntax\": \"sql\",\n",
      "    \"paused\": 0,\n",
      "    \"view_only\": false,\n",
      "    \"type\": \"rds_mysql\",\n",
      "    \"id\": 8\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Datawarehouse\",\n",
      "    \"pause_reason\": null,\n",
      "    \"syntax\": \"sql\",\n",
      "    \"paused\": 0,\n",
      "    \"view_only\": false,\n",
      "    \"type\": \"rds_mysql\",\n",
      "    \"id\": 6\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Dimagh Production\",\n",
      "    \"pause_reason\": null,\n",
      "    \"syntax\": \"sql\",\n",
      "    \"paused\": 0,\n",
      "    \"view_only\": false,\n",
      "    \"type\": \"rds_mysql\",\n",
      "    \"id\": 2\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Ledger Production\",\n",
      "    \"pause_reason\": null,\n",
      "    \"syntax\": \"sql\",\n",
      "    \"paused\": 0,\n",
      "    \"view_only\": false,\n",
      "    \"type\": \"rds_mysql\",\n",
      "    \"id\": 4\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"rider_db_notification\",\n",
      "    \"pause_reason\": null,\n",
      "    \"syntax\": \"sql\",\n",
      "    \"paused\": 0,\n",
      "    \"view_only\": false,\n",
      "    \"type\": \"mysql\",\n",
      "    \"id\": 15\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"rider_db_orders\",\n",
      "    \"pause_reason\": null,\n",
      "    \"syntax\": \"sql\",\n",
      "    \"paused\": 0,\n",
      "    \"view_only\": false,\n",
      "    \"type\": \"mysql\",\n",
      "    \"id\": 14\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"rider_db_routing\",\n",
      "    \"pause_reason\": null,\n",
      "    \"syntax\": \"sql\",\n",
      "    \"paused\": 0,\n",
      "    \"view_only\": false,\n",
      "    \"type\": \"mysql\",\n",
      "    \"id\": 16\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"rider_db_usermgt\",\n",
      "    \"pause_reason\": null,\n",
      "    \"syntax\": \"sql\",\n",
      "    \"paused\": 0,\n",
      "    \"view_only\": false,\n",
      "    \"type\": \"mysql\",\n",
      "    \"id\": 17\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"staging_db_orders\",\n",
      "    \"pause_reason\": null,\n",
      "    \"syntax\": \"sql\",\n",
      "    \"paused\": 0,\n",
      "    \"view_only\": false,\n",
      "    \"type\": \"mysql\",\n",
      "    \"id\": 13\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Whatsapp\",\n",
      "    \"pause_reason\": null,\n",
      "    \"syntax\": \"sql\",\n",
      "    \"paused\": 0,\n",
      "    \"view_only\": false,\n",
      "    \"type\": \"rds_mysql\",\n",
      "    \"id\": 5\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Replace these with your Redash instance details\n",
    "REDASH_HOST = \"https://redash.truckitin.ai\"\n",
    "API_KEY = \"loMLO5S6tcTusPWt5dcExEA4qMaRRQBbrkbcSuLx\"\n",
    "\n",
    "# URL for the data sources API\n",
    "url = f\"{REDASH_HOST}/api/data_sources\"\n",
    "\n",
    "# Headers with the API key\n",
    "headers = {\"Authorization\": f\"Key {API_KEY}\"}\n",
    "\n",
    "# Make the request\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check for a successful response\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data_sources = response.json()\n",
    "    print(json.dumps(data_sources, indent=2))\n",
    "else:\n",
    "    print(f\"Failed to get data sources: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m data_source_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m14\u001b[39m  \u001b[38;5;66;03m# You need to know this beforehand\u001b[39;00m\n\u001b[0;32m     40\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM OrderDetails LIMIT 10\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 42\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_source_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m, in \u001b[0;36mrun_query\u001b[1;34m(api_key, query, data_source_id)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Create and execute the query\u001b[39;00m\n\u001b[0;32m     11\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/queries\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m     14\u001b[0m     json\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: query, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_source_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_source_id},\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 16\u001b[0m query_id \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Execute the query\u001b[39;00m\n\u001b[0;32m     19\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/queries/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/results\u001b[39m\u001b[38;5;124m\"\u001b[39m, headers\u001b[38;5;241m=\u001b[39mheaders)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def run_query(api_key, query, data_source_id):\n",
    "    base_url = \"https://redash.truckitin.ai/api\"\n",
    "    headers = {\"Authorization\": f\"Key {api_key}\"}\n",
    "\n",
    "    # Create and execute the query\n",
    "    response = requests.post(\n",
    "        f\"{base_url}/queries\",\n",
    "        headers=headers,\n",
    "        json={\"query\": query, \"data_source_id\": data_source_id},\n",
    "    )\n",
    "    query_id = response.json()[\"id\"]\n",
    "\n",
    "    # Execute the query\n",
    "    response = requests.post(f\"{base_url}/queries/{query_id}/results\", headers=headers)\n",
    "    job = response.json()[\"job\"]\n",
    "\n",
    "    # Poll for query results\n",
    "    while job[\"status\"] not in (3, 4):  # 3: completed, 4: failed\n",
    "        response = requests.get(f'{base_url}/jobs/{job[\"id\"]}', headers=headers)\n",
    "        job = response.json()[\"job\"]\n",
    "        time.sleep(1)\n",
    "\n",
    "    if job[\"status\"] == 3:\n",
    "        results = requests.get(\n",
    "            f\"{base_url}/queries/{query_id}/results.json\", headers=headers\n",
    "        ).json()[\"query_result\"][\"data\"]\n",
    "        return pd.DataFrame(results[\"rows\"])\n",
    "    else:\n",
    "        raise Exception(\"Query execution failed.\")\n",
    "\n",
    "\n",
    "# Usage example\n",
    "api_key = \"loMLO5S6tcTusPWt5dcExEA4qMaRRQBbrkbcSuLx\"\n",
    "data_source_id = 14  # You need to know this beforehand\n",
    "query = \"SELECT * FROM OrderDetails LIMIT 10\"\n",
    "\n",
    "df = run_query(api_key, query, data_source_id)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected response format: 'query_result'\n",
      "ERROR:root:Query execution failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"job\": {\n",
      "    \"status\": 1,\n",
      "    \"error\": \"\",\n",
      "    \"id\": \"8e3aac56-161e-457b-9ab2-35dd85ad6d34\",\n",
      "    \"query_result_id\": null,\n",
      "    \"updated_at\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "REDASH_HOST = \"https://redash.truckitin.ai\"\n",
    "API_KEY = \"loMLO5S6tcTusPWt5dcExEA4qMaRRQBbrkbcSuLx\"\n",
    "DATA_SOURCE_ID = \"14\"\n",
    "\n",
    "# Ad-hoc SQL query\n",
    "query = \"\"\"\n",
    "    SELECT id,\n",
    "       consignment_id,\n",
    "       origin_city_id,\n",
    "       origin_city_name,\n",
    "       city_id,\n",
    "       dest_city_name,\n",
    "       warehouse_id,\n",
    "       warehouse_title,\n",
    "       area_id,\n",
    "       area_title,\n",
    "       sort_addr_id,\n",
    "       sort_addr_title,\n",
    "       Concat(area_title, ' > ', sort_addr_title) AS L3_L4\n",
    "    FROM   orderdetails\n",
    "    LIMIT 10\"\"\"\n",
    "\n",
    "# URL for running the query\n",
    "url = f\"{REDASH_HOST}/api/query_results\"\n",
    "\n",
    "# Headers with the API key\n",
    "headers = {\"Authorization\": f\"Key {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Payload with the data source ID and query\n",
    "payload = {\"data_source_id\": DATA_SOURCE_ID, \"query\": query}\n",
    "\n",
    "\n",
    "def run_query(url, headers, payload):\n",
    "    try:\n",
    "        # Make the request\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "        # Check for a successful response\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the JSON response\n",
    "        result = response.json()\n",
    "\n",
    "        print(json.dumps(result, indent=2))\n",
    "\n",
    "        # Extract the query results\n",
    "        query_results = result[\"query_result\"][\"data\"][\"rows\"]\n",
    "\n",
    "        # Convert the query results to a Pandas DataFrame\n",
    "        df = pd.DataFrame(query_results)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Request failed: {e}\")\n",
    "        return None\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"Unexpected response format: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Run the query and get the DataFrame\n",
    "df = run_query(url, headers, payload)\n",
    "\n",
    "if df is not None:\n",
    "    logging.info(\"Query executed successfully.\")\n",
    "    # Proceed with further processing on df\n",
    "    print(df.head())\n",
    "else:\n",
    "    logging.error(\"Query execution failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   area_id sort_addr_title consignment_id  city_id origin_city_name  \\\n",
      "0     6209            None      WR4025670      772           Lahore   \n",
      "1     2012            None      WR4025672        1          Karachi   \n",
      "2     6209            None      WR4025673      772       Faisalabad   \n",
      "3      364            None      WR4025674        2           Lahore   \n",
      "4      561            None      WR4025675        3           Lahore   \n",
      "\n",
      "   warehouse_id dest_city_name  origin_city_id   area_title L3_L4  \\\n",
      "0            55        PATOKI                2      Pattoki  None   \n",
      "1             2        Karachi               1      Block 1  None   \n",
      "2            55        PATOKI               10      Pattoki  None   \n",
      "3             3         Lahore               2  Bahria Town  None   \n",
      "4             4      Hyderabad               2        Other  None   \n",
      "\n",
      "   warehouse_title  id sort_addr_id  \n",
      "0  PTI Head Office   1         None  \n",
      "1     LuckyOne Hub   2         None  \n",
      "2  PTI Head Office   3         None  \n",
      "3  LHE Head Office   4         None  \n",
      "4  HDD Head Office   5         None  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "REDASH_HOST = \"https://redash.truckitin.ai\"\n",
    "API_KEY = \"loMLO5S6tcTusPWt5dcExEA4qMaRRQBbrkbcSuLx\"\n",
    "DATA_SOURCE_ID = \"14\"\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT id,\n",
    "           consignment_id,\n",
    "           origin_city_id,\n",
    "           origin_city_name,\n",
    "           city_id,\n",
    "           dest_city_name,\n",
    "           warehouse_id,\n",
    "           warehouse_title,\n",
    "           area_id,\n",
    "           area_title,\n",
    "           sort_addr_id,\n",
    "           sort_addr_title,\n",
    "           CONCAT(area_title, ' > ', sort_addr_title) AS L3_L4\n",
    "    FROM OrderDetails\n",
    "    LIMIT 10\"\"\"\n",
    "\n",
    "headers = {\"Authorization\": f\"Key {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "\n",
    "def submit_query():\n",
    "    url = f\"{REDASH_HOST}/api/query_results\"\n",
    "    payload = {\"data_source_id\": DATA_SOURCE_ID, \"query\": query, \"max_age\": 0}\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"job\"][\"id\"]\n",
    "\n",
    "\n",
    "def check_job_status(job_id):\n",
    "    url = f\"{REDASH_HOST}/api/jobs/{job_id}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"job\"]\n",
    "\n",
    "\n",
    "def get_query_results(query_result_id):\n",
    "    url = f\"{REDASH_HOST}/api/query_results/{query_result_id}.json\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"query_result\"][\"data\"][\"rows\"]\n",
    "\n",
    "\n",
    "def run_query():\n",
    "    try:\n",
    "        job_id = submit_query()\n",
    "        logging.info(f\"Job ID: {job_id}\")\n",
    "\n",
    "        while True:\n",
    "            job = check_job_status(job_id)\n",
    "            status = job[\"status\"]\n",
    "            logging.info(f\"Job Status: {status}\")\n",
    "\n",
    "            if status == 3:  # completed\n",
    "                return get_query_results(job[\"query_result_id\"])\n",
    "            elif status == 4:  # failed\n",
    "                logging.error(\n",
    "                    f\"Query execution failed: {job.get('error', 'No error message provided')}\"\n",
    "                )\n",
    "                return None\n",
    "\n",
    "            # Use ThreadPoolExecutor for non-blocking wait\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                future = executor.submit(check_job_status, job_id)\n",
    "                job = future.result(timeout=1)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Request failed: {e}\")\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"Unexpected response format: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Run the query and get the DataFrame\n",
    "results = run_query()\n",
    "if results:\n",
    "    df = pd.DataFrame(results)\n",
    "    logging.info(\"Query executed successfully.\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    logging.error(\"Query execution failed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
